# LLM (Large Language Model) 

- claude3 GPT-3.5 Gemini 1.0 가격 비교 [링크](https://www.anthropic.com/news/claude-3-haiku)  
- Prompt Enginnering Guide [링크](https://www.promptingguide.ai/)  
- Google DeepMind의 새로운 gemma 모델이 인간 선호도를 학습하기 위해 RLHF용 PPO 대신 REINFORCE를 사용 [링크](https://www.linkedin.com/posts/sararosehooker_were-you-curious-to-find-out-google-deepmind-activity-7166829507855785984-ErXB/?utm_source=share&utm_medium=member_android)  
- groq 클라우드: [링크](https://console.groq.com/)  
  현재 사용 가능한 모델은 mixtral-8x7b과 llama2-70b 이렇게 두가지만 사용 가능
- 대규모 언어 모델을 위한 검색-증강 생성(RAG) 기술 현황 [링크](https://discuss.pytorch.kr/t/rag-1-2/3135)


- RAG From Scratch 강의 [링크](https://www.youtube.com/playlist?list=PLQdCGOoR3OqYQX7DcUVUeAGxZcNqbU0Jn)

- Model & API 비교 [링크](https://artificialanalysis.ai/)

- 초보자를 위한 디코더 유형 트랜스포머 아키텍쳐 개론 [링크](https://cogdex-dtta.streamlit.app/)

- 랭체인(langchain) + 허깅페이스(HuggingFace) 모델 사용법 [링크](https://teddylee777.github.io/langchain/langchain-tutorial-02/)  
- What are Quantized LLMs? [링크](https://www.tensorops.ai/post/what-are-quantized-llms)
- Awesome Model Quantization [링크](https://github.com/htqin/awesome-model-quantization)
- 모두의연구소 - 이준범님 한국어 오픈액세스 언어모델 [링크](https://www.youtube.com/watch?v=ZoMWvu4RsGc)
- 챗봇 아레나 [링크](https://elo.instruct.kr/)
- Inflection 개인용 AI [링크](https://inflection.ai/inflection-2-5)

- 멀티 agent용 langgraph [링크](https://python.langchain.com/docs/langgraph)  
- tgi supported models [링크](https://huggingface.co/docs/text-generation-inference/en/supported_models)  
- vllm supported models[링크](https://docs.vllm.ai/en/latest/models/supported_models.html)  

- LAG for LLMs 참고 논문 링크 [링크](https://www.promptingguide.ai/research/rag)

- 2024 LangCon [링크](https://2024langcon.oopy.io/ad8826ed-57f6-4a98-b864-3d5aadccc5c1)

- 더 빠른 대규모 벡터 연산을 위해 FPGA를 이용해 가속을 해볼까? (feat. MetisX) [링크](https://www.youtube.com/watch?v=0o9T7C_Q8zQ&list=PLqkITFr6P-oQ9YXCr5rsMGS1uyMqwA-H9&index=8) 발표자료 [링크](https://file.notion.so/f/f/259b6cbf-2853-4c70-8dcc-8c636ffcfb99/7343a4b5-6755-4b62-8636-fca736e4b1ad/%EA%B9%80%EC%A3%BC%ED%98%84_%EB%8D%94_%EB%B9%A0%EB%A5%B8_%EB%B2%A1%ED%84%B0_%EC%97%B0%EC%82%B0%EC%9D%84_%EC%9C%84%ED%95%B4_FPGA%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%B4_%EA%B0%80%EC%86%8D%EC%9D%84_%ED%95%B4%EB%B3%BC%EA%B9%8C.pdf?id=4819fd23-8417-461a-9106-7a1c6b777b3b&table=block&spaceId=259b6cbf-2853-4c70-8dcc-8c636ffcfb99&expirationTimestamp=1711180800000&signature=1OJbISWLyt5mNHy04Q4SOQIYoBf1o8s7KsMfVEZM6LY&downloadName=%EA%B9%80%EC%A3%BC%ED%98%84_%EB%8D%94+%EB%B9%A0%EB%A5%B8+%EB%B2%A1%ED%84%B0+%EC%97%B0%EC%82%B0%EC%9D%84+%EC%9C%84%ED%95%B4+FPGA%EB%A5%BC+%EC%9D%B4%EC%9A%A9%ED%95%B4+%EA%B0%80%EC%86%8D%EC%9D%84+%ED%95%B4%EB%B3%BC%EA%B9%8C.pdf) 발표자료2 [링크](https://file.notion.so/f/f/259b6cbf-2853-4c70-8dcc-8c636ffcfb99/33a5221e-88d8-4907-9523-2b765a9109bf/%E1%84%87%E1%85%A1%E1%86%A8%E1%84%8C%E1%85%B5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC.pdf?id=7a9f428b-911b-4160-96b3-d470f73f04ee&table=block&spaceId=259b6cbf-2853-4c70-8dcc-8c636ffcfb99&expirationTimestamp=1711180800000&signature=PU6NUg6Ic1HMwDRIUT1xjWJkmnJjlwxj2a465CT_7sc&downloadName=%E1%84%87%E1%85%A1%E1%86%A8%E1%84%8C%E1%85%B5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC.pdf)

- The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits [링크](https://arxiv.org/abs/2402.17764) 

- 생성 모델 튜닝 어디까지 왔나. [링크](https://youtu.be/jnUeWCe4xTo) 발표자료 [링크](https://file.notion.so/f/f/259b6cbf-2853-4c70-8dcc-8c636ffcfb99/bd6fda6c-7f6d-4e9b-b7a0-68c17e6d8918/%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC_%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF_%E1%84%90%E1%85%B2%E1%84%82%E1%85%B5%E1%86%BC_%E1%84%8B%E1%85%A5%E1%84%83%E1%85%B5%E1%84%81%E1%85%A1%E1%84%8C%E1%85%B5_%E1%84%8B%E1%85%AA%E1%86%BB%E1%84%82%E1%85%A1.pdf?id=47e92d9d-1972-4fa7-b6a8-4aa0066813dd&table=block&spaceId=259b6cbf-2853-4c70-8dcc-8c636ffcfb99&expirationTimestamp=1711180800000&signature=0ZxDGlmqtbTLDuZ6drJqPkaxI9Lo4x4npP4nFxKgwUk&downloadName=%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC+%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF+%E1%84%90%E1%85%B2%E1%84%82%E1%85%B5%E1%86%BC+%E1%84%8B%E1%85%A5%E1%84%83%E1%85%B5%E1%84%81%E1%85%A1%E1%84%8C%E1%85%B5+%E1%84%8B%E1%85%AA%E1%86%BB%E1%84%82%E1%85%A1.pdf)  

- Qdrant 시멘틱 검색을 통한 codebae 탐색 [링크](https://qdrant.tech/documentation/tutorials/code-search/#)

- LLM과 벡터 데이터베이스 통합: 실습 가이드 [링크](https://mlengineering.medium.com/integrating-vector-databases-with-llms-a-hands-on-guide-82d2463114fb)

- 한국어 임베딩 모델 논의 글 [링크](https://arca.live/b/alpaca/76514064)  
- 한국어 임베딩 비교 KorSTS Benchmark [링크](https://github.com/jhgan00/ko-sentence-transformers)

- tesla m10 vs geforce rtx 3090 ti [링크](https://technical.city/ko/video/Tesla-M10-vs-GeForce-RTX-3090-Ti)

- AutoRAG [링크](https://github.com/Marker-Inc-Korea/AutoRAG)

- llmware - RAG을 포함한 LLM 기반 어플리케이션 개발용 통합 프레임워크 [링크](https://github.com/llmware-ai/llmware)


- Langchain Model 컴포넌트와 호출 방법 [링크](https://bcho.tistory.com/1409)

- 긴 문서(long context) 에 대한 참신한 RAG 방법론 [링크](https://www.youtube.com/watch?v=gcdkISrpMCA)

- R.A.G. 우리가 절대 쉽게결과물을 얻을 수 없는 이유 [링크](https://www.youtube.com/watch?v=NfQrRQmDrcc)

- ChatGPT가 프롬프트 중간 정보를 활용하지 못한다고?! [링크](https://lilys.ai/digest/366009?sId=DjAWtzqKIEo)

- GenerativeAI Examples [링크](https://github.com/NVIDIA/GenerativeAIExamples/blob/main/docs/README.md#getting-started)

- Anthropic의 Claude 3 Haiku 모델이 이제 Amazon Bedrock에서 이용 가능 [링크](https://aws.amazon.com/ko/blogs/aws/anthropics-claude-3-haiku-model-is-now-available-in-amazon-bedrock/)
- claude 프롬프트 엔지니어링 소개 [링크](https://docs.anthropic.com/claude/docs/prompt-engineering)
- openai 프롬프트 엔지니어링 소개 [링크](https://platform.openai.com/docs/guides/prompt-engineering)  
- 프롬프트 엔지니어링이란? [링크](https://moon-walker.medium.com/the-art-of-prompt-engneering-1-prompt-engineering%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80-4a7a88ce67c) 

- langchain 튜토리얼 [링크](https://wikidocs.net/book/14314
)

- LangChain으로 Multimodal과 RAG를 구현 [링크](https://github.com/kyopark2014/llm-multimodal-and-rag)  
- AI 팅커러스 시애틀에서 Excel(스프레드시트)로 구현한 GPT2 [링크](https://spreadsheets-are-all-you-need.ai/)  요약본[링크](https://lilys.ai/digest/382714?sId=AIACopCeGlQ&source=video&result=summaryNote&isBlogRequested=false&s=1)

- HuggingFace 임베딩 모델을 Amazon SageMaker에 배포하고 Llama-Index와 함께 사용하기 [링크](https://community.aws/content/2cZgUY8U4st0bR6VAoY8BxhsXzu/deploying-an-huggingface-embedding-model-to-amazon-sagemaker-and-consuming-it-with-llama-index?lang=en)  

-  "Two Agents Debate with Tools"(도구를 활용한 토론 에이전트) [링크](https://www.youtube.com/watch?v=NaU89YXQAoI)

- CHAT GPT 원하는 형식의 응답 얻기 [링크](https://www.semrush.com/blog/chatgpt-prompts/)  

- Delimiter를 활용한 Prompt Formatting [링크](https://velog.io/@samuel_cogdex/Delimiter%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-Prompt-Formatting-%EA%B8%B0%EB%B2%95-%EC%A0%95%ED%99%95%ED%95%9C-%EC%A7%80%EC%8B%9C%EC%82%AC%ED%95%AD-%EC%A0%84%EB%8B%AC%EC%9D%98-%ED%95%B5%EC%8B%AC)
- 응답형식 노하우 정리 [링크](gpt_result_format_prompt.md)
- 투디지트의 한글 LLM 공개 자료 [링크](https://github.com/davidkim205/nox)
- 투디지트의 한글 LLM 리더보드 [링크](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)

- langchain 반말 코드 예시 [링크](https://smith.langchain.com/hub/pwoc517/non-polite_non-honorific_korean?organizationId=8b51adb9-c530-5edf-93c6-b6d05bf3fc3f)
- langchain 존댓말 코드 예시 [링크](https://smith.langchain.com/hub/pwoc517/polite_honorific_korean?organizationId=8b51adb9-c530-5edf-93c6-b6d05bf3fc3f)  

- RAT(Retrieval Augmented Thoughts) 논문 리뷰 [링크](https://hyun941213.tistory.com/entry/RAT-Retrieval-Augmented-Thoughts-ElicitContext-Aware-Reasoning-in-Long-HorizonGeneration-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)

- NLP paper implementation relevant to classification with PyTorch [링크](https://github.com/seopbo/nlp_classification/tree/master)  

- A survey on complex factual question answering [링크](https://www.sciencedirect.com/science/article/pii/S2666651022000249?via%3Dihub)


- RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation [링크](https://openreview.net/forum?id=mlJLVigNHp&referrer=%5Bthe%20profile%20of%20Weijia%20Shi%5D(%2Fprofile%3Fid%3D~Weijia_Shi1))


- SuRe: Improving Open-domain Question Answering of LLMs via Summarized Retrieval [링크](https://openreview.net/forum?id=w4DW6qkRmt)


- Learning to Filter Context for Retrieval-Augmented Generation [링크](https://openreview.net/forum?id=6WI0lVIDdyZ)


## model list
- OLMo: Open Language Model[링크](https://github.com/allenai/OLMo)  
  OLMo is a repository for training and using AI2's state-of-the-art open language models. It is built by scientists, for scientists.  
- Synatra-kiqu-10.7B  [링크](https://huggingface.co/maywell/Synatra-kiqu-10.7B)  
  한국어 모델 
- solar:  [링크](https://ollama.com/library/solar)  
  A compact, yet powerful 10.7B large language model designed for single-turn conversation. 

- Orion-14B [링크](https://github.com/OrionStarAI/Orion)  
  Orion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  

- kf-deberta-multitask [링크](https://github.com/upskyy/kf-deberta-multitask)  
  kakaobank의 kf-deberta-base 모델을 KorNLI, KorSTS 데이터셋으로 파인튜닝한 모델.

- Mamba-ko-2.8B [링크](https://huggingface.co/kuotient/mamba-ko-2.8b)  
  Mamba-ko-2.8B is the state space model, further pretrained(or continous trained) with synthetically generated dataset - korean_textbooks.

- grok-1 [링크](https://github.com/xai-org/grok-1)  
  파라미터 크기는 8x39B 이고 활성 파라미터 크기는 86B 모델 전체 용량은 296.4GB  
  This repository contains JAX example code for loading and running the Grok-1 open-weights model.  

- Yi-Ko-DUS-9B [링크](https://huggingface.co/beomi/Yi-Ko-DUS-9B)  
  Yi-Ko-DUS model serves as DUS-applied and advanced iterations of beomi/Yi-Ko-6B model, benefiting from an expanded vocabulary and the inclusion of Korean/English corpus in its further pretraining.

- Polyglot-Ko-12.8B [링크](https://huggingface.co/EleutherAI/polyglot-ko-12.8b)  
  Polyglot-Ko is a series of large-scale Korean autoregressive language models made by the EleutherAI polyglot team.
- open-llama-2-ko-7b [링크](https://huggingface.co/beomi/open-llama-2-ko-7b)  
  Open-Llama-2-Ko represents an advanced iteration of the Llama 2 model, featuring an expanded vocabulary and the inclusion of a Korean corpus for enhanced pretraining  

- Llama2-7b-ko-Fine-tuning [링크](https://github.com/SEUNKOREA/Llama2-7b-ko-Fine-tuning?tab=readme-ov-file)  
  한국어 사전학습 모델 Llama2-ko-7b를 한국어 ML Comprehenzsion 데이터셋, KorQuAD 2.0로 4bit QLoRA를 적용해서 Fine-tuning 하는 코드  

- best open-source embedding models [링크](https://python.langchain.com/docs/integrations/text_embedding/bge_huggingface)

- T5-Large  
- Vicuna  
- Bloom  

## Vector Database
- 백터 DB 비교(FAISS 없음) [링크](https://discuss.pytorch.kr/t/2023-picking-a-vector-database-a-comparison-and-guide-for-2023/2625)
- FAISS와 Milvus 속도, mAP 비교 [링크](https://github.com/milvus-io/milvus/discussions/4939)

- FAISS Milvus benchmarking [링크](https://github.com/milvus-io/milvus/discussions/4939)



## DataSet
- 한국문화에 대한 QA 데이터셋인 CLIcK 논문 [링크](https://arxiv.org/abs/2403.06412) 데이터셋 [링크](https://github.com/rladmstn1714/CLIcK)  

- 한국어 데이터 말뭉치 [링크](https://kli.korean.go.kr/corpus/request/corpusRegist.do)

- gutenberg-dpo [링크](https://huggingface.co/datasets/jondurbin/gutenberg-dpo-v0.1/viewer/default/train)